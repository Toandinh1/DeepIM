{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Toandinh1/DeepIM/blob/master/scratchpad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIYdn1woOS1n",
        "outputId": "d53505eb-97c6-4032-fa7e-2bbdacb04cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.5`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from scipy.special import binom\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.python.ops.gen_math_ops import sign\n",
        "#from keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "N = 4 # number of sub-carriers\n",
        "K = 2 # number of active sub-carriers\n",
        "M = 4 # M-ary modulation order\n",
        "\n",
        "SNRdb = 15 # Training SNR\n",
        "\n",
        "traing_epochs = 100\n",
        "l_rate = 0.1\n",
        "total_batch = 20 # number of batches per epoch\n",
        "batch_size = 100\n",
        "\n",
        "\n",
        "n_hidden_1 = 16\n",
        "n_hidden_2 = 32\n",
        "n_hidden_3 = 32\n",
        "n_hidden_4 = 16\n",
        "n_input = 3*N\n",
        "n_input_2 = 2*N + 2\n",
        "\n",
        "\n",
        "m = int(np.log2(M))\n",
        "c = int(np.log2(binom(N,K)))\n",
        "q = K*m + c # number of bits per OFDM-IM symbol\n",
        "Q= 2**q\n",
        "n_output = c\n",
        "n_output_2 = 4\n",
        "c1 = 1\n",
        "c2 = 0.25\n",
        "\n",
        "pi = 3.14\n",
        "Gl = 1\n",
        "F = 2\n",
        "d1 = 100\n",
        "d2 = 400\n",
        "PL1 = (4*pi*d1)/(np.sqrt(Gl)*((3*10^8)/F*1000))\n",
        "PL2 = (4*pi*d2)/(np.sqrt(Gl)*((3*10^8)/F*1000))\n",
        "\n",
        "display_step = 5\n",
        "SNR = 10**(SNRdb/10)\n",
        "sigma = np.sqrt(1/SNR)\n",
        "qam_factor = (2/3)*(M-1)\n",
        "\n",
        "\n",
        "a = 1/np.sqrt(2)\n",
        "\n",
        "# M-ary modulations\n",
        "if M==4:\n",
        "    QAM = np.array([1+1j, 1-1j, -1+1j, -1-1j], dtype=complex) # gray mapping\n",
        "elif M==8:\n",
        "    QAM = np.array([1, a+a*1j, -a+a*1j, 1j, a-a*1j, -1j, -1, -a-a*1j], dtype=complex) # 8PSK, not 8QAM indeed\n",
        "    qam_factor = 1\n",
        "elif M==16:\n",
        "    QAM = np.array([-3+3j, -3+1j, -3-3j, -3-1j, \n",
        "                    -1+3j, -1+1j, -1-3j, -1-1j, \n",
        "                    3+3j, 3+1j, 3-3j, 3-1j, \n",
        "                    1+3j, 1+1j, 1-3j, 1-1j], dtype=complex)\n",
        "else:\n",
        "    QAM = np.array([1, -1], dtype=complex) #BPSK\n",
        "    qam_factor = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# index patterns for N=4 and K=1,2,3 only\n",
        "if K==1:\n",
        "    idx = np.array([[0],[1],[2],[3]])\n",
        "elif K==2:\n",
        "    idx = np.array([[0,1],[2,3],[0,2],[1,3]]) \n",
        "else:\n",
        "    idx = np.array([[0,1,2],[1,2,3],[0,2,3],[0,1,3]]) \n",
        "def SC_IM_NO_train(bit1,bit2, SNRdb):\n",
        "        #user1\n",
        "    bit_id1 = bit1[0:c:1]\n",
        "    id_de1 = bit_id1.dot(2**np.arange(bit_id1.size)[::-1])\n",
        "    bit_sy1 = bit1[c:q:1]   \n",
        "    bit_K1 = bit_sy1.reshape(-1,m)\n",
        "    sy_de1 = np.zeros((K,), dtype=int)\n",
        "    sym1 = np.zeros((K,), dtype=complex)\n",
        "    for i in range(K):\n",
        "        bit_sy_i1 = bit_K1[i,:]\n",
        "        sy_de1[i] = bit_sy_i1.dot(2**np.arange(bit_sy_i1.size)[::-1])\n",
        "        sym1[i] = QAM[sy_de1[i]]\n",
        "\n",
        "    tx_sym1 = np.zeros((N,), dtype=complex)\n",
        "    tx_sym1[idx[id_de1,:]] = sym1\n",
        "    tx_sym1 = tx_sym1*np.sqrt(c1)\n",
        "  #user2\n",
        "    bit_id2 = bit2[0:c:1]\n",
        "    id_de2 = bit_id2.dot(2**np.arange(bit_id2.size)[::-1])\n",
        "    bit_sy2 = bit2[c:q:1]   \n",
        "    bit_K2 = bit_sy2.reshape(-1,m)\n",
        "    sy_de2 = np.zeros((K,), dtype=int)\n",
        "    sym2 = np.zeros((K,), dtype=complex)\n",
        "    for i in range(K):\n",
        "        bit_sy_i2 = bit_K2[i,:]\n",
        "        sy_de2[i] = bit_sy_i2.dot(2**np.arange(bit_sy_i2.size)[::-1])\n",
        "        sym2[i] = QAM[sy_de2[i]]\n",
        "\n",
        "    tx_sym2 = np.zeros((N,), dtype=complex)\n",
        "    tx_sym2[idx[id_de2,:]] = sym2\n",
        "    tx_sym2 = tx_sym2*np.sqrt(c2)\n",
        "\n",
        "    #transmision\n",
        "    SNR = 10**(SNRdb/10)\n",
        "    sigma = np.sqrt(1/SNR)\n",
        "    noise = sigma*np.sqrt(1/2)*(np.random.randn(*tx_sym1.shape)+1j*np.random.randn(*tx_sym1.shape))\n",
        "    H1 = 4*np.sqrt(1/2)*(np.random.randn(*tx_sym1.shape)+1j*np.random.randn(*tx_sym1.shape))\n",
        "    H2 = 1*np.sqrt(1/2)*(np.random.randn(*tx_sym1.shape)+1j*np.random.randn(*tx_sym1.shape))\n",
        "    Y1 = H1*np.sqrt(c1)*tx_sym1\n",
        "    Y2 = H2*np.sqrt(c2)*tx_sym2\n",
        "    \n",
        "    Y = H1*np.sqrt(c1)*tx_sym1 + H2*np.sqrt(c2)*tx_sym2 + noise\n",
        "     \n",
        "    y_bar = Y/(np.sqrt(c1)*H1)\n",
        "    y_con = np.concatenate((np.real(y_bar),np.imag(y_bar)))\n",
        "    y_m = np.absolute(Y1)\n",
        "    y_con1 = np.concatenate((y_con,y_m))\n",
        "    Ysic = Y - y_bar*H1*np.sqrt(c1)\n",
        "    y2_ = Ysic/(np.sqrt(c2)*H2)\n",
        "    y2_m = np.absolute(Y2)\n",
        "    y2__ = np.concatenate((np.real(y2_),np.imag(y2_)))\n",
        "    y_con2 =np.concatenate((y2__,y2_m))\n",
        "    Y_con1= y_con1.reshape(n_input,)\n",
        "    y_2 = y_con.reshape( n_input_2-2,)\n",
        "  \n",
        "    \n",
        "    return y_con1,y_2\n",
        "\n",
        "def SC_IM_NO_test(bit1, bit2, SNRdb):\n",
        "        #user1\n",
        "    bit_id1 = bit1[0:c:1]\n",
        "    id_de1 = bit_id1.dot(2**np.arange(bit_id1.size)[::-1])\n",
        "    bit_sy1 = bit1[c:q:1]   \n",
        "    bit_K1 = bit_sy1.reshape(-1,m)\n",
        "    sy_de1 = np.zeros((K,), dtype=int)\n",
        "    sym1 = np.zeros((K,), dtype=complex)\n",
        "    for i in range(K):\n",
        "        bit_sy_i1 = bit_K1[i,:]\n",
        "        sy_de1[i] = bit_sy_i1.dot(2**np.arange(bit_sy_i1.size)[::-1])\n",
        "        sym1[i] = QAM[sy_de1[i]]\n",
        "\n",
        "    tx_sym1 = np.zeros((N,), dtype=complex)\n",
        "    tx_sym1[idx[id_de1,:]] = sym1\n",
        "    tx_sym1 = tx_sym1*np.sqrt(c1)\n",
        "  #user2\n",
        "    bit_id2 = bit2[0:c:1]\n",
        "    id_de2 = bit_id2.dot(2**np.arange(bit_id2.size)[::-1])\n",
        "    bit_sy2 = bit2[c:q:1]   \n",
        "    bit_K2 = bit_sy2.reshape(-1,m)\n",
        "    sy_de2 = np.zeros((K,), dtype=int)\n",
        "    sym2 = np.zeros((K,), dtype=complex)\n",
        "    for i in range(K):\n",
        "        bit_sy_i2 = bit_K2[i,:]\n",
        "        sy_de2[i] = bit_sy_i2.dot(2**np.arange(bit_sy_i2.size)[::-1])\n",
        "        sym2[i] = QAM[sy_de2[i]]\n",
        "\n",
        "    tx_sym2 = np.zeros((N,), dtype=complex)\n",
        "    tx_sym2[idx[id_de2,:]] = sym2\n",
        "    tx_sym2 = tx_sym2*np.sqrt(c2)\n",
        "\n",
        "    #transmision\n",
        "    SNR = 10**(SNRdb/10)\n",
        "    sigma = np.sqrt(1/SNR)\n",
        "    noise = sigma*np.sqrt(1/2)*(np.random.randn(*tx_sym1.shape)+1j*np.random.randn(*tx_sym1.shape))\n",
        "    H1 = 4*np.sqrt(1/2)*(np.random.randn(*tx_sym1.shape)+1j*np.random.randn(*tx_sym1.shape))\n",
        "    H2 = 1*np.sqrt(1/2)*(np.random.randn(*tx_sym1.shape)+1j*np.random.randn(*tx_sym1.shape))\n",
        "    Y1 = H1*np.sqrt(c1)*tx_sym1\n",
        "    Y2 = H2*np.sqrt(c2)*tx_sym2\n",
        "    \n",
        "    Y = H1*np.sqrt(c1)*tx_sym1 + H2*np.sqrt(c2)*tx_sym2 + noise\n",
        "     \n",
        "    y_bar = Y/(np.sqrt(c1)*H1)\n",
        "    y_con = np.concatenate((np.real(y_bar),np.imag(y_bar)))\n",
        "    y_m = np.absolute(Y1)\n",
        "    y_con1 = np.concatenate((y_con,y_m))\n",
        "    Ysic = Y - y_bar*H1*np.sqrt(c1)\n",
        "    y2_ = Ysic/(np.sqrt(c2)*H2)\n",
        "    y2_m = np.absolute(Y2)\n",
        "    y2__ = np.concatenate((np.real(y2_),np.imag(y2_)))\n",
        "    y_con2 =np.concatenate((y2__,y2_m))\n",
        "    Y_con1= y_con1.reshape(n_input,)\n",
        "    y_2 = y_con.reshape(n_input_2 - 2,)\n",
        "  \n",
        "    \n",
        "    return y_con1,y_2\n",
        "\n"
      ],
      "metadata": {
        "id": "JMmeYn8dYP4D"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(32,3,activation='relu',input_shape=(12,1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64,activation='tanh'))\n",
        "model.add(Dense(2,activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "d79tMnbNY7M5",
        "outputId": "259aa63a-02d8-42e7-8690-8706c807b8e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_4 (Conv1D)            (None, 10, 32)            128       \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 320)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 64)                20544     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 20,802\n",
            "Trainable params: 20,802\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"mse\",optimizer='adam')\n",
        "for epoch in range(traing_epochs):\n",
        "  input_samples = []\n",
        "  input_labels = []\n",
        "  for index_k in range(0, 10000):\n",
        "    bits = np.random.binomial(n=1,p=0.5,size=(q,))\n",
        "    signal_output,signal_out_2 = SC_IM_NO_train(bits,bits,SNRdb)\n",
        "    input_labels.append(bits[0:c:1])\n",
        "    input_samples.append(signal_output)\n",
        "\n",
        "  batch_x = np.asarray(input_samples)\n",
        "  batch_x = batch_x.reshape(10000,12,1)\n",
        "  batch_y = np.asarray(input_labels)\n",
        "  model.fit(batch_x,batch_y,epochs=1)"
      ],
      "metadata": {
        "id": "vc0QcnrSZDYV",
        "outputId": "8f2b0316-12e2-4ba4-c53b-626e99e9f16c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 2s 181us/sample - loss: 4.2299e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 3.2163e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 3.4006e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 65us/sample - loss: 2.5287e-06\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 3.9256e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 3.7256e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 62us/sample - loss: 9.8829e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 2.4024e-06\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 66us/sample - loss: 3.8331e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 9.0879e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 63us/sample - loss: 2.5940e-06\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 115us/sample - loss: 8.2744e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 6.7556e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 4.9045e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 1.5861e-06\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 4.4090e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 1.8708e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 3.8757e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 65us/sample - loss: 2.2366e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 70us/sample - loss: 5.7001e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 1.3150e-06\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 9.4744e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 7.5046e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 64us/sample - loss: 5.0653e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 7.7555e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 1.0039e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 1.6820e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 3.6412e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 9.5182e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 1.9887e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 3.1124e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 1.9326e-04\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 1.7640e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 67us/sample - loss: 2.0300e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 65us/sample - loss: 8.2864e-06\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 69us/sample - loss: 3.1727e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 65us/sample - loss: 9.5583e-06\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 63us/sample - loss: 5.5027e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 5.8782e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 9.7267e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 64us/sample - loss: 2.0868e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 65us/sample - loss: 2.1937e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 4.1808e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 4.0406e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 66us/sample - loss: 4.9259e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 2.7831e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 63us/sample - loss: 5.7309e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 7.8762e-06\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 2.8397e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 63us/sample - loss: 1.0878e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 1.8828e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 64us/sample - loss: 3.7085e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 7.0060e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 65us/sample - loss: 2.1132e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 63us/sample - loss: 2.4026e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 1.0699e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 5.8651e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 9.7564e-06\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 63us/sample - loss: 5.8648e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 2.1461e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 52us/sample - loss: 1.5840e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 1.4092e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 66us/sample - loss: 4.6421e-06\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 3.1621e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 5.7428e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 4.7725e-06\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 2.7762e-06\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 1.5567e-04\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 7.8385e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 1.3094e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 7.7703e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 6.6998e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 4.1253e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 3.4964e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 1.1989e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 8.5026e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 1.9458e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 8.9163e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 65us/sample - loss: 4.9861e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 69us/sample - loss: 3.3066e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 63us/sample - loss: 2.0096e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 85us/sample - loss: 1.6106e-04\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: 4.2394e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 63us/sample - loss: 1.2994e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 2.3542e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 2.2561e-06\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 2.1958e-09\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 6.2550e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 1.0311e-04\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 2.2318e-06\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 5.1146e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 2.1887e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 3.7006e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 56us/sample - loss: 1.5330e-07\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 5.7604e-06\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 55us/sample - loss: 9.9060e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 54us/sample - loss: 1.5125e-08\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 53us/sample - loss: 4.9635e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 64us/sample - loss: 4.8035e-05\n",
            "Train on 10000 samples\n",
            "10000/10000 [==============================] - 1s 60us/sample - loss: 1.3468e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#=========Training DNNs=============\n",
        "\n",
        "X = tf.placeholder(\"float\",[None, n_input_2])\n",
        "Y = tf.placeholder(\"float\",[None, n_output_2])\n",
        "initializer = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "def encoder(x):\n",
        "    weights = {\n",
        "        'encoder_h1': tf.Variable(initializer([n_input_2, n_hidden_1])),\n",
        "        'encoder_h2': tf.Variable(initializer([n_hidden_1, n_output_2])),\n",
        "    }\n",
        "    \n",
        "    biases = {\n",
        "        'encoder_b1': tf.Variable(initializer([n_hidden_1])),\n",
        "        'encoder_b2': tf.Variable(initializer([n_output_2])),\n",
        "    }\n",
        "    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
        "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
        "\n",
        "    return layer_2\n",
        "\n",
        "y_pred = encoder(X)\n",
        "y_true = Y\n",
        "\n",
        "cost = tf.reduce_mean(tf.pow(y_true - y_pred,2))\n",
        "learning_rate = tf.placeholder(tf.float32,shape=[])\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "def frange(x,y,jump):\n",
        "    while x < y:\n",
        "        yield x\n",
        "        x +=jump\n",
        "\n",
        "EbNodB_range = list(frange(0,50,5))\n",
        "ber_info = [None]*len(EbNodB_range)\n",
        "ber_index = [None]*len(EbNodB_range)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  #Training\n",
        "    sess.run(init)\n",
        "    for epoch in range(traing_epochs):\n",
        "        avg_cost = 0\n",
        "        for index_m in range(total_batch):\n",
        "            input_samples = []\n",
        "            input_labels = []\n",
        "            input_samples_2 = []\n",
        "            input_labels_2 = []\n",
        "            for index_k in range(0, batch_size):\n",
        "                bits = np.random.binomial(n=1,p=0.5,size=(q,))\n",
        "                signal_output,signal_output_2 = SC_IM_NO_train(bits,bits,SNRdb)\n",
        "                input_labels.append(bits[0:c:1])\n",
        "                input_samples.append(signal_output)\n",
        "                input_labels_2.append(bits[c::1])\n",
        "                input_samples_2.append(signal_output_2)\n",
        "\n",
        "            batch_x = np.asarray(input_samples)\n",
        "            batch_y = np.asarray(input_labels_2)\n",
        "            batch_x_2 = np.asarray(input_samples_2)\n",
        "            batch_x = batch_x.reshape(batch_size,12,1)\n",
        "            u_index = model.predict(batch_x)\n",
        "            batch_x_2 = np.concatenate((u_index,batch_x_2),axis=1)\n",
        "            \n",
        "\n",
        "            _,cs = sess.run([optimizer,cost], feed_dict={X:batch_x_2,\n",
        "                                                        Y:batch_y,\n",
        "                                                        learning_rate:l_rate})\n",
        "            avg_cost += cs / total_batch\n",
        "        if epoch % display_step == 0:\n",
        "            print(\"Epoch:\",'%04d' % (epoch+1), \"cost=\", \\\n",
        "               \"{:.9f}\".format(avg_cost))\n",
        "  #==========Testing=============\n",
        "    for n in range(0,len(EbNodB_range)):\n",
        "      input_samples_test = []\n",
        "      input_labels_test = []\n",
        "      input_samples_test_2 = []\n",
        "      input_labels_test_2 = []\n",
        "      for i in range(0, 100000):\n",
        "        bits = np.random.binomial(n=1, p=0.5, size=(q, )) \n",
        "        signal_output,signal_output_2 = SC_IM_NO_test(bits,bits, EbNodB_range[n])\n",
        "        input_labels_test.append(bits[0:c:1])\n",
        "        input_samples_test.append(signal_output)\n",
        "        input_labels_test_2.append(bits[c::1])\n",
        "        input_samples_test_2.append(signal_output_2)\n",
        "                \n",
        "      batch_x_test = np.asarray(input_samples_test)\n",
        "      batch_x_test_2 = np.asarray(input_samples_test_2)\n",
        "      batch_y_test = np.asarray(input_labels_test_2)\n",
        "      batch_y_test_2 = np.asarray(input_labels_test)\n",
        "      batch_x_test = batch_x_test.reshape(100000,12,1)\n",
        "      yhat = model.predict(batch_x_test)\n",
        "      batch_x_test_2 = np.concatenate((yhat,batch_x_test_2),1)\n",
        "\n",
        "      mean_error_rate = 1-tf.reduce_mean(tf.reduce_mean(tf.to_float(tf.equal(tf.sign(y_pred-0.5), tf.cast(tf.sign(batch_y_test-0.5),tf.float32))),1))\n",
        "      ber_info[n]  = mean_error_rate.eval({X:batch_x_test_2})\n",
        "      ber_index[n] = 1-tf.reduce_mean(tf.reduce_mean(tf.cast(tf.equal(tf.sign(yhat-0.5),tf.cast(tf.sign(batch_y_test_2-0.5),tf.float32)),tf.float32),1))\n",
        "      "
      ],
      "metadata": {
        "id": "AqZdFKCwaSkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ber_info"
      ],
      "metadata": {
        "id": "v93vjpH_vny4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oWlFwRX5lklL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ber_index"
      ],
      "metadata": {
        "id": "SATGLUQjwWQr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}